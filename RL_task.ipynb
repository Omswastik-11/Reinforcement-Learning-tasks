{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5762449d-7685-4c37-8a7f-8738e149a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a480e671-c6b0-48bd-9ead-db2d466ed2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeXEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(MazeXEnv, self).__init__()\n",
    "        self.maze = np.array([\n",
    "            [0,0,0,1,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,0,1],\n",
    "            [0,0,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,1],\n",
    "            [0,1,1,1,1,1,1,1,0,1,1,1,0,1,0,1,0,1,1,1],\n",
    "            [0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1],\n",
    "            [0,1,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,0,1],\n",
    "            [0,0,0,1,0,1,0,0,0,0,0,1,0,1,0,1,0,0,0,1],\n",
    "            [1,1,1,1,0,1,1,1,1,1,0,1,0,1,0,1,0,1,0,1],\n",
    "            [0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,1,0,1],\n",
    "            [0,1,1,1,0,1,1,1,1,1,0,1,0,0,0,1,0,1,0,1],\n",
    "            [0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1],\n",
    "            [0,1,0,1,1,1,1,1,1,1,1,1,0,1,0,1,1,1,0,1],\n",
    "            [0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1],\n",
    "            [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        ])\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (0, 4)\n",
    "        self.checkpoints = [(14, 6), (4, 10), (2, 16), (10, 2)]\n",
    "        self.state = self.start\n",
    "        self.observation_space = spaces.Box(low=0, high=max(self.maze.shape)-1, shape=(2,), dtype=np.int32)\n",
    "        self.action_space = spaces.Discrete(4)  # 0: move straight, 1: right, 2: left, 3: backward\n",
    "\n",
    "        # Pygame setup\n",
    "        self.screen_width = 800\n",
    "        self.screen_height = 600\n",
    "        self.screen_size = (self.screen_width, self.screen_height)\n",
    "        self.cell_size = min(self.screen_width // self.maze.shape[1], self.screen_height // self.maze.shape[0])\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode(self.screen_size)\n",
    "        pygame.display.set_caption('MazeX')\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def step(self, action, reward):\n",
    "        current_row, current_col = self.state\n",
    "        next_state = list(self.state)\n",
    "        \n",
    "        if action == 0:  # Move straight\n",
    "            # Check if moving straight stays within bounds and doesn't hit a wall\n",
    "            if current_col + 1 < self.maze.shape[1] and self.maze[current_row, current_col + 1] == 0:\n",
    "                next_state[1] += 1\n",
    "        elif action == 1:  # Move right\n",
    "            if current_row + 1 < self.maze.shape[0] and self.maze[current_row + 1, current_col] == 0:\n",
    "                next_state[0] += 1\n",
    "        elif action == 2:  # Move left\n",
    "            if current_row - 1 >= 0 and self.maze[current_row - 1, current_col] == 0:\n",
    "                next_state[0] -= 1\n",
    "        elif action == 3:  # Move backward\n",
    "            if current_col - 1 >= 0 and self.maze[current_row, current_col - 1] == 0:\n",
    "                next_state[1] -= 1\n",
    "        \n",
    "        reward += -1  # Each step costs -1\n",
    "         # Checking if the next state is different from the current state or not\n",
    "        if next_state != list(self.state):\n",
    "            self.state = tuple(next_state)\n",
    "            done = self.state == self.goal\n",
    "            if self.state in self.checkpoints:\n",
    "                reward += 30 # if checkpoit reached then 30 reward points added\n",
    "            if done:\n",
    "                reward += 100 # if goal reached then 100 reward points added\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        return np.array(self.state), reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        self.screen.fill((255, 255, 255))  # White background\n",
    "\n",
    "        # Draw the maze\n",
    "        for row in range(self.maze.shape[0]):\n",
    "            for col in range(self.maze.shape[1]):\n",
    "                color = (255, 255, 255) # white colour for paths\n",
    "                if self.maze[row, col] == 1:\n",
    "                    color = (0, 0, 0)  # black colour for Walls\n",
    "                pygame.draw.rect(self.screen, color, \n",
    "                                 pygame.Rect(col * self.cell_size, row * self.cell_size, self.cell_size, self.cell_size))\n",
    "\n",
    "        # Draw the start position as green colour\n",
    "        pygame.draw.rect(self.screen, (0, 255, 0), \n",
    "                         pygame.Rect(self.start[1] * self.cell_size, self.start[0] * self.cell_size, self.cell_size, self.cell_size))\n",
    "\n",
    "        # Draw the goal position as red colour\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), \n",
    "                         pygame.Rect(self.goal[1] * self.cell_size, self.goal[0] * self.cell_size, self.cell_size, self.cell_size))\n",
    "\n",
    "        # Draw checkpoints as blue colour\n",
    "        for cp in self.checkpoints:\n",
    "            pygame.draw.rect(self.screen, (0, 0, 255), \n",
    "                             pygame.Rect(cp[1] * self.cell_size, cp[0] * self.cell_size, self.cell_size, self.cell_size))\n",
    "\n",
    "        # Draw the robot as a yellow coloured square\n",
    "        pygame.draw.rect(self.screen, (255, 255, 0), \n",
    "                         pygame.Rect(self.state[1] * self.cell_size, self.state[0] * self.cell_size, self.cell_size, self.cell_size))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8797dcb-b31b-47b6-8628-77c03e06d92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -1\n",
      "Reward: -2\n",
      "Reward: -3\n",
      "Reward: -4\n",
      "Reward: -5\n",
      "Reward: -6\n",
      "Reward: -7\n",
      "Reward: -8\n",
      "Reward: -9\n",
      "Reward: -10\n",
      "Reward: -11\n",
      "Reward: -12\n",
      "Reward: -13\n",
      "Reward: -14\n",
      "Reward: -15\n",
      "Reward: -16\n",
      "Reward: -17\n",
      "Reward: -18\n",
      "Reward: -19\n",
      "Reward: -20\n",
      "Reward: -21\n",
      "Reward: -22\n",
      "Reward: -23\n",
      "Reward: -24\n",
      "Reward: -25\n",
      "Reward: -26\n",
      "Reward: -27\n",
      "Reward: -28\n",
      "Reward: -29\n",
      "Reward: -30\n",
      "Reward: -31\n",
      "Reward: -32\n",
      "Reward: -33\n",
      "Reward: -34\n",
      "Reward: -35\n",
      "Reward: -36\n",
      "Reward: -37\n",
      "Reward: -38\n",
      "Reward: -39\n",
      "Reward: -40\n",
      "Reward: -41\n",
      "Reward: -42\n",
      "Reward: -43\n",
      "Reward: -44\n",
      "Reward: -45\n",
      "Reward: -46\n",
      "Reward: -47\n",
      "Reward: -48\n",
      "Reward: -49\n",
      "Reward: -50\n",
      "Reward: -51\n",
      "Reward: -52\n",
      "Reward: -53\n",
      "Reward: -54\n",
      "Reward: -55\n",
      "Reward: -56\n",
      "Reward: -57\n",
      "Reward: -58\n",
      "Reward: -59\n",
      "Reward: -60\n",
      "Reward: -61\n",
      "Reward: -62\n",
      "Reward: -63\n",
      "Reward: -64\n",
      "Reward: -65\n",
      "Reward: -66\n",
      "Reward: -67\n",
      "Reward: -68\n",
      "Reward: -69\n",
      "Reward: -70\n",
      "Reward: -71\n",
      "Reward: -72\n",
      "Reward: -73\n",
      "Reward: -74\n",
      "Reward: -75\n",
      "Reward: -76\n",
      "Reward: -77\n",
      "Reward: -78\n",
      "Reward: -79\n",
      "Reward: -80\n",
      "Reward: -81\n",
      "Reward: -82\n",
      "Reward: -83\n",
      "Reward: -84\n",
      "Reward: -85\n",
      "Reward: -86\n",
      "Reward: -87\n",
      "Reward: -88\n",
      "Reward: -89\n",
      "Reward: -90\n",
      "Reward: -91\n",
      "Reward: -92\n",
      "Reward: -93\n",
      "Reward: -94\n",
      "Reward: -95\n",
      "Reward: -96\n",
      "Reward: -97\n",
      "Reward: -98\n",
      "Reward: -99\n",
      "Reward: -100\n",
      "Reward: -101\n",
      "Reward: -102\n",
      "Reward: -103\n",
      "Reward: -104\n",
      "Reward: -105\n",
      "Reward: -106\n",
      "Reward: -107\n",
      "Reward: -108\n",
      "Reward: -109\n",
      "Reward: -110\n",
      "Reward: -111\n",
      "Reward: -112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# this produces some random actions but using Q-learning we can produce desired actions also to maximize rewards\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action,reward)\n\u001b[1;32m---> 12\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[50], line 103\u001b[0m, in \u001b[0;36mMazeXEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     99\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \n\u001b[0;32m    100\u001b[0m                  pygame\u001b[38;5;241m.\u001b[39mRect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size))\n\u001b[0;32m    102\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m--> 103\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = MazeXEnv()\n",
    "state = env.reset()\n",
    "done = False\n",
    "reward = 0\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            done = True\n",
    "            break\n",
    "    action = env.action_space.sample()  # this produces some random actions but using Q-learning we can produce desired actions also to maximize rewards\n",
    "    state, reward, done, _ = env.step(action,reward)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d498e57-ba69-4904-9478-5e89eb30fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed823d-912d-4092-a808-caef85ccfa2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
